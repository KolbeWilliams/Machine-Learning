{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJEBweaGLR1_",
        "outputId": "c5c2b907-e542-4eb1-cabc-c7cf84915720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.62596094\n",
            "Iteration 2, loss = 0.71545183\n",
            "Iteration 3, loss = 0.39763243\n",
            "Iteration 4, loss = 0.34187725\n",
            "Iteration 5, loss = 0.30905766\n",
            "Iteration 6, loss = 0.26742603\n",
            "Iteration 7, loss = 0.21947432\n",
            "Iteration 8, loss = 0.20074529\n",
            "Iteration 9, loss = 0.18187897\n",
            "Iteration 10, loss = 0.14773919\n",
            "Iteration 11, loss = 0.14567455\n",
            "Iteration 12, loss = 0.17789234\n",
            "Iteration 13, loss = 0.11458189\n",
            "Iteration 14, loss = 0.12139887\n",
            "Iteration 15, loss = 0.09648850\n",
            "Iteration 16, loss = 0.12569404\n",
            "Iteration 17, loss = 0.09845633\n",
            "Iteration 18, loss = 0.09126164\n",
            "Iteration 19, loss = 0.07871848\n",
            "Iteration 20, loss = 0.06313710\n",
            "Accuracy for model 1: 0.9047619047619048\n",
            "\n",
            "Iteration 1, loss = 1.95408499\n",
            "Iteration 2, loss = 1.92106494\n",
            "Iteration 3, loss = 1.92359231\n",
            "Iteration 4, loss = 1.91400625\n",
            "Iteration 5, loss = 1.91212608\n",
            "Iteration 6, loss = 1.91225533\n",
            "Iteration 7, loss = 1.91739016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 1.91167232\n",
            "Iteration 9, loss = 1.89976151\n",
            "Iteration 10, loss = 1.88443704\n",
            "Iteration 11, loss = 1.85712041\n",
            "Iteration 12, loss = 1.78203006\n",
            "Iteration 13, loss = 1.63881391\n",
            "Iteration 14, loss = 1.46083414\n",
            "Iteration 15, loss = 1.32379456\n",
            "Iteration 16, loss = 1.25637859\n",
            "Iteration 17, loss = 1.20098781\n",
            "Iteration 18, loss = 1.16315510\n",
            "Iteration 19, loss = 1.13076149\n",
            "Iteration 20, loss = 1.10753899\n",
            "Accuracy for model 2: 0.7380952380952381\n",
            "\n",
            "Iteration 1, loss = 1.89207996\n",
            "Iteration 2, loss = 1.16306525\n",
            "Iteration 3, loss = 0.91075371\n",
            "Iteration 4, loss = 0.80290338\n",
            "Iteration 5, loss = 0.73612506\n",
            "Iteration 6, loss = 0.69043313\n",
            "Iteration 7, loss = 0.65544669\n",
            "Iteration 8, loss = 0.62312465\n",
            "Iteration 9, loss = 0.58881186\n",
            "Iteration 10, loss = 0.56548314\n",
            "Iteration 11, loss = 0.53972423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 12, loss = 0.51544305\n",
            "Iteration 13, loss = 0.49414746\n",
            "Iteration 14, loss = 0.47659199\n",
            "Iteration 15, loss = 0.45361313\n",
            "Iteration 16, loss = 0.43573859\n",
            "Iteration 17, loss = 0.42182748\n",
            "Iteration 18, loss = 0.40117757\n",
            "Iteration 19, loss = 0.39683584\n",
            "Iteration 20, loss = 0.38009727\n",
            "Iteration 21, loss = 0.36508400\n",
            "Iteration 22, loss = 0.35629705\n",
            "Iteration 23, loss = 0.34720803\n",
            "Iteration 24, loss = 0.33068557\n",
            "Iteration 25, loss = 0.32185474\n",
            "Iteration 26, loss = 0.31481724\n",
            "Iteration 27, loss = 0.31081827\n",
            "Iteration 28, loss = 0.30221400\n",
            "Iteration 29, loss = 0.29682050\n",
            "Iteration 30, loss = 0.28143648\n",
            "Iteration 31, loss = 0.28015985\n",
            "Iteration 32, loss = 0.27259717\n",
            "Iteration 33, loss = 0.26332812\n",
            "Iteration 34, loss = 0.26009308\n",
            "Iteration 35, loss = 0.24807075\n",
            "Iteration 36, loss = 0.23974096\n",
            "Iteration 37, loss = 0.22795356\n",
            "Iteration 38, loss = 0.21777685\n",
            "Iteration 39, loss = 0.21237494\n",
            "Iteration 40, loss = 0.21221033\n",
            "Iteration 41, loss = 0.22420217\n",
            "Iteration 42, loss = 0.24520432\n",
            "Iteration 43, loss = 0.22003574\n",
            "Iteration 44, loss = 0.25349824\n",
            "Iteration 45, loss = 0.22131584\n",
            "Iteration 46, loss = 0.18631544\n",
            "Iteration 47, loss = 0.18655895\n",
            "Iteration 48, loss = 0.17374645\n",
            "Iteration 49, loss = 0.17285338\n",
            "Iteration 50, loss = 0.16872499\n",
            "Iteration 51, loss = 0.16039231\n",
            "Iteration 52, loss = 0.15857879\n",
            "Iteration 53, loss = 0.15303496\n",
            "Iteration 54, loss = 0.14952425\n",
            "Iteration 55, loss = 0.15429279\n",
            "Iteration 56, loss = 0.14957507\n",
            "Iteration 57, loss = 0.14880651\n",
            "Iteration 58, loss = 0.14124280\n",
            "Iteration 59, loss = 0.14277225\n",
            "Iteration 60, loss = 0.16736175\n",
            "Iteration 61, loss = 0.16943285\n",
            "Iteration 62, loss = 0.13793190\n",
            "Iteration 63, loss = 0.14345357\n",
            "Iteration 64, loss = 0.13107281\n",
            "Iteration 65, loss = 0.12967762\n",
            "Iteration 66, loss = 0.12813661\n",
            "Iteration 67, loss = 0.12525188\n",
            "Iteration 68, loss = 0.12156217\n",
            "Iteration 69, loss = 0.11130783\n",
            "Iteration 70, loss = 0.12256248\n",
            "Iteration 71, loss = 0.11251668\n",
            "Iteration 72, loss = 0.10929632\n",
            "Iteration 73, loss = 0.12187517\n",
            "Iteration 74, loss = 0.10185682\n",
            "Iteration 75, loss = 0.10295908\n",
            "Iteration 76, loss = 0.09837707\n",
            "Iteration 77, loss = 0.09661385\n",
            "Iteration 78, loss = 0.09622079\n",
            "Iteration 79, loss = 0.09432514\n",
            "Iteration 80, loss = 0.10081554\n",
            "Iteration 81, loss = 0.09652490\n",
            "Iteration 82, loss = 0.09392736\n",
            "Iteration 83, loss = 0.08861288\n",
            "Iteration 84, loss = 0.08871349\n",
            "Iteration 85, loss = 0.08893826\n",
            "Iteration 86, loss = 0.08693649\n",
            "Iteration 87, loss = 0.08342589\n",
            "Iteration 88, loss = 0.07983969\n",
            "Iteration 89, loss = 0.07729318\n",
            "Iteration 90, loss = 0.08984501\n",
            "Iteration 91, loss = 0.07935128\n",
            "Iteration 92, loss = 0.07135114\n",
            "Iteration 93, loss = 0.07575333\n",
            "Iteration 94, loss = 0.10306750\n",
            "Iteration 95, loss = 0.06960752\n",
            "Iteration 96, loss = 0.12177178\n",
            "Iteration 97, loss = 0.07558158\n",
            "Iteration 98, loss = 0.06799195\n",
            "Iteration 99, loss = 0.06627707\n",
            "Iteration 100, loss = 0.06407796\n",
            "Iteration 101, loss = 0.06193595\n",
            "Iteration 102, loss = 0.05892983\n",
            "Iteration 103, loss = 0.05721344\n",
            "Iteration 104, loss = 0.05706457\n",
            "Iteration 105, loss = 0.05557811\n",
            "Iteration 106, loss = 0.05661897\n",
            "Iteration 107, loss = 0.05676075\n",
            "Iteration 108, loss = 0.05585520\n",
            "Iteration 109, loss = 0.06201069\n",
            "Iteration 110, loss = 0.07736099\n",
            "Iteration 111, loss = 0.07658908\n",
            "Iteration 112, loss = 0.06700185\n",
            "Iteration 113, loss = 0.06485587\n",
            "Iteration 114, loss = 0.04780945\n",
            "Iteration 115, loss = 0.05348214\n",
            "Iteration 116, loss = 0.04508187\n",
            "Iteration 117, loss = 0.04746021\n",
            "Iteration 118, loss = 0.04300849\n",
            "Iteration 119, loss = 0.04337145\n",
            "Iteration 120, loss = 0.04257150\n",
            "Iteration 121, loss = 0.04138798\n",
            "Iteration 122, loss = 0.04464375\n",
            "Iteration 123, loss = 0.03992974\n",
            "Iteration 124, loss = 0.04003293\n",
            "Iteration 125, loss = 0.03785628\n",
            "Iteration 126, loss = 0.03743135\n",
            "Iteration 127, loss = 0.03699207\n",
            "Iteration 128, loss = 0.03637300\n",
            "Iteration 129, loss = 0.03552059\n",
            "Iteration 130, loss = 0.03548913\n",
            "Iteration 131, loss = 0.03446651\n",
            "Iteration 132, loss = 0.03386844\n",
            "Iteration 133, loss = 0.03334000\n",
            "Iteration 134, loss = 0.03280975\n",
            "Iteration 135, loss = 0.03236813\n",
            "Iteration 136, loss = 0.03215756\n",
            "Iteration 137, loss = 0.03118585\n",
            "Iteration 138, loss = 0.03117138\n",
            "Iteration 139, loss = 0.03033409\n",
            "Iteration 140, loss = 0.03147262\n",
            "Iteration 141, loss = 0.02942427\n",
            "Iteration 142, loss = 0.02937914\n",
            "Iteration 143, loss = 0.02899609\n",
            "Iteration 144, loss = 0.02816485\n",
            "Iteration 145, loss = 0.02805358\n",
            "Iteration 146, loss = 0.02760096\n",
            "Iteration 147, loss = 0.02703009\n",
            "Iteration 148, loss = 0.02696093\n",
            "Iteration 149, loss = 0.02702777\n",
            "Iteration 150, loss = 0.02589240\n",
            "Iteration 151, loss = 0.02584155\n",
            "Iteration 152, loss = 0.02545295\n",
            "Iteration 153, loss = 0.02506115\n",
            "Iteration 154, loss = 0.02626049\n",
            "Iteration 155, loss = 0.02531662\n",
            "Iteration 156, loss = 0.02481055\n",
            "Iteration 157, loss = 0.02422714\n",
            "Iteration 158, loss = 0.02346168\n",
            "Iteration 159, loss = 0.02393401\n",
            "Iteration 160, loss = 0.02339314\n",
            "Iteration 161, loss = 0.02287185\n",
            "Iteration 162, loss = 0.02214216\n",
            "Iteration 163, loss = 0.02164757\n",
            "Iteration 164, loss = 0.02140751\n",
            "Iteration 165, loss = 0.02109712\n",
            "Iteration 166, loss = 0.02094609\n",
            "Iteration 167, loss = 0.02046118\n",
            "Iteration 168, loss = 0.02037873\n",
            "Iteration 169, loss = 0.02008313\n",
            "Iteration 170, loss = 0.01973760\n",
            "Iteration 171, loss = 0.01998594\n",
            "Iteration 172, loss = 0.01930425\n",
            "Iteration 173, loss = 0.01909275\n",
            "Iteration 174, loss = 0.01912830\n",
            "Iteration 175, loss = 0.01897643\n",
            "Iteration 176, loss = 0.01856846\n",
            "Iteration 177, loss = 0.01819415\n",
            "Iteration 178, loss = 0.01851229\n",
            "Iteration 179, loss = 0.01872835\n",
            "Iteration 180, loss = 0.01799887\n",
            "Iteration 181, loss = 0.01775814\n",
            "Iteration 182, loss = 0.01744174\n",
            "Iteration 183, loss = 0.01721621\n",
            "Iteration 184, loss = 0.01698971\n",
            "Iteration 185, loss = 0.01675216\n",
            "Iteration 186, loss = 0.01662400\n",
            "Iteration 187, loss = 0.01650120\n",
            "Iteration 188, loss = 0.01635706\n",
            "Iteration 189, loss = 0.01611469\n",
            "Iteration 190, loss = 0.01642516\n",
            "Iteration 191, loss = 0.01569388\n",
            "Iteration 192, loss = 0.01561099\n",
            "Iteration 193, loss = 0.01546147\n",
            "Iteration 194, loss = 0.01514730\n",
            "Iteration 195, loss = 0.01612057\n",
            "Iteration 196, loss = 0.01556688\n",
            "Iteration 197, loss = 0.01504211\n",
            "Iteration 198, loss = 0.01498316\n",
            "Iteration 199, loss = 0.01445889\n",
            "Iteration 200, loss = 0.01440065\n",
            "Iteration 201, loss = 0.01424077\n",
            "Iteration 202, loss = 0.01411924\n",
            "Iteration 203, loss = 0.01402193\n",
            "Iteration 204, loss = 0.01385024\n",
            "Iteration 205, loss = 0.01369250\n",
            "Iteration 206, loss = 0.01358905\n",
            "Iteration 207, loss = 0.01363942\n",
            "Iteration 208, loss = 0.01329263\n",
            "Iteration 209, loss = 0.01328282\n",
            "Iteration 210, loss = 0.01316592\n",
            "Iteration 211, loss = 0.01312906\n",
            "Iteration 212, loss = 0.01299488\n",
            "Iteration 213, loss = 0.01284063\n",
            "Iteration 214, loss = 0.01267596\n",
            "Iteration 215, loss = 0.01266484\n",
            "Iteration 216, loss = 0.01250500\n",
            "Iteration 217, loss = 0.01236989\n",
            "Iteration 218, loss = 0.01229679\n",
            "Iteration 219, loss = 0.01226778\n",
            "Iteration 220, loss = 0.01204701\n",
            "Iteration 221, loss = 0.01193887\n",
            "Iteration 222, loss = 0.01183697\n",
            "Iteration 223, loss = 0.01171988\n",
            "Iteration 224, loss = 0.01177966\n",
            "Iteration 225, loss = 0.01154643\n",
            "Iteration 226, loss = 0.01145130\n",
            "Iteration 227, loss = 0.01141038\n",
            "Iteration 228, loss = 0.01122619\n",
            "Iteration 229, loss = 0.01116741\n",
            "Iteration 230, loss = 0.01117613\n",
            "Iteration 231, loss = 0.01100521\n",
            "Iteration 232, loss = 0.01094377\n",
            "Iteration 233, loss = 0.01086415\n",
            "Iteration 234, loss = 0.01072738\n",
            "Iteration 235, loss = 0.01084809\n",
            "Iteration 236, loss = 0.01060085\n",
            "Iteration 237, loss = 0.01058839\n",
            "Iteration 238, loss = 0.01106141\n",
            "Iteration 239, loss = 0.01062125\n",
            "Iteration 240, loss = 0.01033388\n",
            "Iteration 241, loss = 0.01020983\n",
            "Iteration 242, loss = 0.01027412\n",
            "Iteration 243, loss = 0.01004581\n",
            "Iteration 244, loss = 0.01011864\n",
            "Iteration 245, loss = 0.00989123\n",
            "Iteration 246, loss = 0.00986475\n",
            "Iteration 247, loss = 0.00969671\n",
            "Iteration 248, loss = 0.00976061\n",
            "Iteration 249, loss = 0.00972215\n",
            "Iteration 250, loss = 0.00949010\n",
            "Iteration 251, loss = 0.00940865\n",
            "Iteration 252, loss = 0.00932750\n",
            "Iteration 253, loss = 0.00925484\n",
            "Iteration 254, loss = 0.00929992\n",
            "Iteration 255, loss = 0.00923264\n",
            "Iteration 256, loss = 0.00904628\n",
            "Iteration 257, loss = 0.00902457\n",
            "Iteration 258, loss = 0.00894440\n",
            "Iteration 259, loss = 0.00888427\n",
            "Iteration 260, loss = 0.00898178\n",
            "Iteration 261, loss = 0.00887622\n",
            "Iteration 262, loss = 0.00867767\n",
            "Iteration 263, loss = 0.00866053\n",
            "Iteration 264, loss = 0.00854661\n",
            "Iteration 265, loss = 0.00848920\n",
            "Iteration 266, loss = 0.00844266\n",
            "Iteration 267, loss = 0.00839633\n",
            "Iteration 268, loss = 0.00832607\n",
            "Iteration 269, loss = 0.00827276\n",
            "Iteration 270, loss = 0.00821589\n",
            "Iteration 271, loss = 0.00820301\n",
            "Iteration 272, loss = 0.00837519\n",
            "Iteration 273, loss = 0.00827631\n",
            "Iteration 274, loss = 0.00805963\n",
            "Iteration 275, loss = 0.00793716\n",
            "Iteration 276, loss = 0.00797179\n",
            "Iteration 277, loss = 0.00798645\n",
            "Iteration 278, loss = 0.00784813\n",
            "Iteration 279, loss = 0.00775649\n",
            "Iteration 280, loss = 0.00770516\n",
            "Iteration 281, loss = 0.00772786\n",
            "Iteration 282, loss = 0.00760042\n",
            "Iteration 283, loss = 0.00757296\n",
            "Iteration 284, loss = 0.00751555\n",
            "Iteration 285, loss = 0.00745990\n",
            "Iteration 286, loss = 0.00742687\n",
            "Iteration 287, loss = 0.00739102\n",
            "Iteration 288, loss = 0.00738412\n",
            "Iteration 289, loss = 0.00733395\n",
            "Iteration 290, loss = 0.00728829\n",
            "Iteration 291, loss = 0.00724282\n",
            "Iteration 292, loss = 0.00715442\n",
            "Iteration 293, loss = 0.00708914\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Accuracy for model 3: 0.9761904761904762\n",
            "\n",
            "The best model based off of accuracy score is: model 3 with an accuracy score of 0.9761904761904762\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "seeds = open('seeds_dataset.txt', 'r')\n",
        "temp_file = open('temp_file.txt', 'w')\n",
        "with seeds, temp_file:\n",
        "    for line in seeds:\n",
        "        area, perimeter, compactness, length_of_kernel, width_of_kernel, asymmetry_coefficient, length_of_kernel_groove, target = line.split()\n",
        "        new_record = ','.join([area, perimeter, compactness, length_of_kernel, width_of_kernel, asymmetry_coefficient, length_of_kernel_groove, target])\n",
        "        temp_file.write(new_record + '\\n')\n",
        "df = pd.read_csv('temp_file.txt', header=None)\n",
        "x = np.array(df.iloc[:, :-1])\n",
        "y = np.array(df.iloc[:, -1])\n",
        "\n",
        "scaler = StandardScaler().fit(x)\n",
        "x_scaled = scaler.transform(x)\n",
        "\n",
        "y_2d = y.reshape(-1, 1)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_encoded = encoder.fit_transform(y_2d)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "accuracy_arr = []\n",
        "\n",
        "model1 = MLPClassifier(\n",
        "  hidden_layer_sizes = (1000, 100, 300),\n",
        "  activation = 'relu',\n",
        "  solver = 'adam',\n",
        "  max_iter = 20,\n",
        "  batch_size = 16,\n",
        "  alpha = 0.0001,\n",
        "  random_state = 42,\n",
        "  verbose = True,\n",
        "  early_stopping = False)\n",
        "model1.fit(x_train, y_train)\n",
        "\n",
        "pred1 = model1.predict(x_test)\n",
        "accuracy1 = accuracy_score(y_test, pred1)\n",
        "print(f'Accuracy for model 1: {accuracy1}\\n')\n",
        "accuracy_arr.append(accuracy1)\n",
        "\n",
        "model2 = MLPClassifier(\n",
        "  hidden_layer_sizes = (8, 64, 64, 64),\n",
        "  activation = 'logistic',\n",
        "  solver = 'adam',\n",
        "  max_iter = 20,\n",
        "  batch_size = 8,\n",
        "  alpha = 0.0001,\n",
        "  random_state = 42,\n",
        "  verbose = True,\n",
        "  early_stopping = False)\n",
        "model2.fit(x_train, y_train)\n",
        "\n",
        "pred2 = model2.predict(x_test)\n",
        "accuracy2 = accuracy_score(y_test, pred2)\n",
        "print(f'Accuracy for model 2: {accuracy2}\\n')\n",
        "accuracy_arr.append(accuracy2)\n",
        "\n",
        "model3 = MLPClassifier(\n",
        "  hidden_layer_sizes = (128, 64, 32, 16),\n",
        "  activation = 'tanh',\n",
        "  solver = 'adam',\n",
        "  max_iter = 500,\n",
        "  batch_size =32,\n",
        "  alpha = 0.0001,\n",
        "  random_state = 42,\n",
        "  verbose = True,\n",
        "  early_stopping = False)\n",
        "model3.fit(x_train, y_train)\n",
        "\n",
        "pred3 = model3.predict(x_test)\n",
        "accuracy3 = accuracy_score(y_test, pred3)\n",
        "print(f'Accuracy for model 3: {accuracy3}\\n')\n",
        "accuracy_arr.append(accuracy3)\n",
        "\n",
        "best_index = accuracy_arr.index(max(accuracy_arr))\n",
        "print(f'The best model based off of accuracy score is: model {best_index + 1} with an accuracy score of {accuracy_arr[best_index]}')"
      ]
    }
  ]
}